# training options
model_type: EENDC
gradclip: 5
batchsize: 32
max_epochs: 50
hidden_size: 256
num_frames: 500
num_speakers: 3
optimizer: noam
lr: 1.0
loss_factor: "1_1_1"
gradient_accumulation_steps: 1
transformer_encoder_n_heads: 4
transformer_encoder_n_layers: 4
transformer_encoder_dropout: 0.1
noam_warmup_steps: 100000
seed: 777
gpu: 4
