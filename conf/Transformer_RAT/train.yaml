# training options
sampling_rate: 8000
frame_size: 200
frame_shift: 80
model_type: Transformer_RAT
gradclip: 5
batchsize: 32
hidden_size: 256
num_frames: 500
num_speakers: 3
input_transform: logmel23_mn
optimizer: adam
lr: 1e-4
context_size: 7
subsampling: 10
window_size: 5
gradient_accumulation_steps: 2
transformer_encoder_n_heads: 4
transformer_encoder_n_layers: 4
transformer_encoder_dropout: 0.1
noam_warmup_steps: 0.2
seed: 777
gpu: 4